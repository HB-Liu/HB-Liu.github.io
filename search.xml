<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>图论——概率方法</title>
      <link href="/probility/"/>
      <url>/probility/</url>
      
        <content type="html"><![CDATA[<h3 id="图论笔记——概率方法"><a href="#图论笔记——概率方法" class="headerlink" title="图论笔记——概率方法"></a>图论笔记——概率方法</h3><h4 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h4><p>证明存在性，有两种思路：</p><ul><li>构造性：直接构造出需要证明存在的目标</li><li>非构造性：往往很难直接构造，所以采用非构造性方法，比如归纳等；概率方法就属于非构造性方法</li></ul><p>概率方法 (<strong>probabilistic method</strong>) 是解决许多离散的数学问题的有效工具。</p><p>概率方法按往往以下流程作用：尝试证明具有某种良好性质的结构存在，可以<strong>定义一个恰当的概率空间</strong>，接下来去<strong>证明该结构在该概率空间存在的概率大于0，那么该结构一定存在</strong></p><blockquote><p><strong>生日悖论问题</strong></p><p>23 个人里有两个生日相同的人的概率有多大呢？居然有 50%。不计特殊的年月，如 2 月 29 日。于是一年中有 N = 365 天。设房间里有 n 个人，要计算所有人的生日都不相同的概率。那么第一个人的生日是 365 选 365，第二个人是 365 选 364，第三个人 365 选 363 …… 第 n 个人的生日是 365 选 365-(n-1)。所以所有人生日都不相同的概率为<br>$$<br>1-P=\frac{365}{365} \times \frac{364}{365} \times \cdots \times \frac{365-n+1}{365}=\frac{365 !}{365^{n}(365-n) !}<br>$$<br>这里 n! 表示 n 的阶乘。那么，n 个人中有至少两个人生日相同的概率就是 P。将 n 与 P 的关系列入下表：</p><p><img src="https://i.loli.net/2020/07/05/eNHlcAX85Ebf9mh.png"  style="zoom:50%;" /></p><p>可以看到 n = 100 远小于 N = 365 时，就已经几乎必然有一对生日相同，所有人生日两两不同的概率仅一千万分之三。</p></blockquote><hr><h4 id="2-最小支配集"><a href="#2-最小支配集" class="headerlink" title="2. 最小支配集"></a>2. 最小支配集</h4><p>支配集 (<strong>Dominating set</strong>) ：对于一个无向图 $G(V,E)$，支配集 $U\subseteq V$, 它使得集合 $V-U$ 中的所有顶点都至少有一个邻居是 $U$ 中的点</p><p>对于以下无向图，$\{v_4,v_5\}$ 为一个支配集，其中顶点 $\{v_1,v_2,v_3\}$ 被 $v_4$ 支配，$v_6$ 被 $v_5$ 支配</p><p><img src="https://i.loli.net/2020/07/05/hVaZceS1qpJLoXk.png" style="zoom:40%;" /></p><p>在图论中，我们往往对<strong>最小支配集</strong>感兴趣，有以下定理</p><blockquote><p><strong>最小支配集存在性定理</strong>：对于图 $G(V,E), |V|=n$, 它的最小度 $\delta &gt; 1$. 那么图 $G$ 一定存在一个最多有 $n[1+ln(\delta+1)]/(\delta+1)$ 顶点的支配集。</p></blockquote><blockquote><p><strong>证明</strong>：令 $X$ 为随机选取的一个顶点子集合（每个顶点以随机概率 $p\in[0,1]$ 选取），$Y=Yx$ 为点集合 $V-X$ 中没被 $X$ 支配的点构成的集合。接下来计算 $X,Y$ 的数学期望：<br>$$<br>E(|X|) = E(\sum I_v) = n·p<br>$$<br>其中 $I_v$ 为指示变量，表示是否被选中</p><p>对于任意 $v \in Y$, $Pr[v \in Y] = Pr[v 和它所有 neighbors 不在X中] = (1-p)^{d(v)+1} \leq (1-p)^{\delta +1}$ . </p><p>$E(|Y|)$ 可写作一系列指示变量的和：$E(|Y|) = \sum X_v$,  其中 $X_v=1$ if $v$ in $Y$, otherwise $X_v=0$</p><p>则：<br>$$<br>E(|X|+|Y|)\leq np + n(1-p)^{\delta+1}<br>$$</p><p>集合 $U=X \cup Y$ 显然是图 $G$ 的一个支配集.</p><p>以上推论对于所有 $p\in [0,1]$ 都是成立的。我们用上式对 $p$ 求导，为了方便计算，我们使用估计式：$1-p \leq e^{-p}$, 则可以得到精简式：<br>$$<br>E(|U|) = E(|X| + |Y|) \leq np + ne^{-p(\delta+1)}<br>$$<br>对 $p$ 求导并等于 0 解得：<br>$$<br>p ^{*} = \frac{ln(\delta+1)}{\delta+1}<br>$$<br>将最优值带入 $|U|$ 中，得：<br>$$<br>E(|U|) \leq n[1+ln(\delta+1)]/(\delta+1)<br>$$<br>证毕。</p></blockquote><p>注：最小支配集也可以使用<strong>贪心算法</strong>进行求解（在贪心算法章节有详细介绍）</p><hr><h4 id="3-核心思想"><a href="#3-核心思想" class="headerlink" title="3. 核心思想"></a>3. 核心思想</h4><p>(1) 数学期望的<strong>线性</strong>性： $E(|X|+|Y|) = E(X) + E(Y)$, $X$ 和 $Y$ 不独立时候也成立。$\rightarrow$ 善用指示变量</p><p>(2) <strong>可变</strong>性质 （<strong>Alteration</strong> principle）：支配集不是直接得到的，而是通过不断修改 $X$ （将 $Y$ 并入），最后得到支配集 $X$。在概率方法中，可变可通过添加(支配集问题)或者删除(Zarankiewicz问题)得到。</p><p>(3) 最优的 $p$ 值选择：顶点选择的概率 $p$ 起初是无法直接确定的。概率方法的思想就是将 $p$ 作为结果的参数，最后进行优化得到 $p^{*}$。 优化的过程中求解到最优解的精确解可能非常困难，可以去寻找一个次优解/近似解（ $1-p\leq e^{-p}$）</p><hr><h4 id="4-Zarankiewicz问题"><a href="#4-Zarankiewicz问题" class="headerlink" title="4. Zarankiewicz问题"></a>4. Zarankiewicz问题</h4><p>Zarankiewicz问题：如何使得 $n\times n$ 矩阵中不存在 $a \times a$ 的全 1 子矩阵</p><p>在双计数章节中，我们已经用双计数的思想解决了这个问题，接下来我们使用概率方法求解。</p><blockquote><p>概率方法求解：对于 $n \times n $ 的 0-1矩阵，出现 $a\times a$ 全 1 子矩阵的数量的数学期望为：<br>$$<br>E = \left(\begin{array}{l}<br>n \\<br>a<br>\end{array}\right)\left(\begin{array}{l}<br>n \\<br>a<br>\end{array}\right) · p^{a^2}<br>$$<br>矩阵中元素为 $1$ 的元素的数量的数学期望为：<br>$$<br>n^2·p<br>$$</p><p>其中 $p$ 为矩阵中元素为 $1$ 的概率。</p><p>而我们不希望出现全 1 子矩阵，我们要通过“可变性质”来对全 1 子矩阵进行变换，即通过“删除”全1子矩阵中的某个元素 $1$ 来实现。之后，矩阵中剩余的 $1$ 的数量的期望为：<br>$$<br>n^2 ·p -\left(\begin{array}{l}<br>n \\<br>a<br>\end{array}\right)\left(\begin{array}{l}<br>n \\<br>a<br>\end{array}\right) · p^{a^2}<br>$$<br>该结果就是矩阵中为 1 的元素的数量的一个<strong>上界</strong>。</p></blockquote><hr><h4 id="5-独立数-Independent-set"><a href="#5-独立数-Independent-set" class="headerlink" title="5. 独立数 Independent set"></a>5. 独立数 Independent set</h4><p>Recall：独立数 $\alpha(G)$ 和色素数量 $\chi(G)$ 的关系： $\chi(G)·\alpha(G) \geq n$</p><blockquote><p><strong>定理</strong>：若图 $G(V,E)$ 有 $n$ 个顶点和 $\frac{nd}{2}$ 条边($d\geq 1$), 那么必存在一个独立集，它包含的顶点数(独立数) $\alpha(G)\geq \frac{n}{2d}$</p></blockquote><blockquote><p><strong>证明</strong>：首先构造子集合 $X$， 每个顶点被选进集合 $X$ 的概率为 $p$, 则集合 $X$ 的顶点数量的数学期望为：<br>$$<br>E(|X|) = n·p<br>$$<br>设集合 $X$ 中的边为 $e(x,y),x\in X, y \in X$, 即存在连线的两个顶点 $x,y$ 均被选入到集合 $X$ 中，概率为 $p^2$。 易知集合 $X$ 中边数量的数学期望为：<br>$$<br>E(|e|) = \frac{nd}{2} p^2<br>$$<br>要使得 $X$ 为独立集，我们只需要删除 $e(x,y)$ 上的任一顶点，<strong>最多需要</strong>删除 $E(|e|)$ 个顶点（因为有些顶点可能在多条边上）， 则独立集的大小的数学期望为：<br>$$<br>E(\alpha) \geq np - \frac{nd}{2}p^2<br>$$<br>要使得该期望值最大，只需对 $p$ 求导取 0 ， 解得 $p^<em> = \frac{1}{d}$, 带入 $E(\alpha)$, 得到：<br>$$<br>E(\alpha)^</em> \geq \frac{n}{2d}<br>$$</p></blockquote><p>注：在证明该问题中，用到的也是“删除”的思想</p><hr><h4 id="6-二分子图-Bipartite-subgraph"><a href="#6-二分子图-Bipartite-subgraph" class="headerlink" title="6. 二分子图 Bipartite subgraph"></a>6. 二分子图 Bipartite subgraph</h4><blockquote><p><strong>定理</strong>： 若 $G(V,E)$ 为一个有 $n$ 个顶点和 $e$ 条边的图，那么 $G$ 必存在一个至少包含 $\frac{e}{2}$ 条边的二分子图 （注：这里的二分子图并不是严格意义上的二分图）</p></blockquote><blockquote><p><strong>证明一（概率方法）</strong> ：首先构造随机子集合 $X$, 每个点被选入到 $X$ 的概率为 $\frac{1}{2}$, 则被选入 $V-X$ 的概率也为 $\frac{1}{2}$</p><p>对于一条边 $e(x,y)$, 它为一条交叉的边的概率为 $2 · \frac{1}{2} ·\frac{1}{2} = \frac{1}{2}$， 共有 $n$ 条边</p><p>则二分子图的边数 $\frac{e}{2}$</p><p><strong>证明二 （启发式算法）</strong>：从图 $G$ 的一个任意划分 $V(G)$ 开始，它将图 $G$ 划分为两个子集 $X$ 和 $Y$ 。使用在每个子集都有一个端点的边来得到一个二分子图 $H$ 和它的两部分子集合 $X$ 和 $Y$</p><p>假设二分子图 $H$ 中某个顶点 $v\in X$, 它的度 $d_H(v)&lt;\frac{d_G(v)}{2}$, 那么我们就将它从 $X$ 移动到 $Y$, 这样能够使得 $d_H(v) &gt; \frac{d_G(v)}{2}$。 </p><p><img src="https://i.loli.net/2020/07/05/gRmI4YjadM5xlXF.png"  style="zoom:50%;" /></p><p>我们为 $H$ 中所有的点重复这个过程，最后则有 $\forall v\in H, d_H(v) &gt; \frac{d_G(H)}{2}$。 最后根据握手定理我们可以得到：<br>$$<br>e(H) &gt; \frac{e(G)}{2}<br>$$</p></blockquote><p>有关该问题的一些讨论：<a href="https://mathoverflow.net/questions/90343/how-to-proof-every-loopless-graph-g-has-a-bipartite-subgraph-with-at-least-eg" target="_blank" rel="noopener">https://mathoverflow.net/questions/90343/how-to-proof-every-loopless-graph-g-has-a-bipartite-subgraph-with-at-least-eg</a></p><hr><h4 id="7-团数-Clique-number"><a href="#7-团数-Clique-number" class="headerlink" title="7.  团数 Clique number"></a>7.  团数 Clique number</h4><p>团数 $w(G)$ $\leftrightarrow$ 独立数 $\alpha(G)$：</p><p>由团和独立集的关系可知，团数可以通过求解其补图 (补图即不相连的边转换为连接，相连的边删除) 的独立数得到。</p><p><img src="https://i.loli.net/2020/07/05/IqSRJP8e1bDNim7.png" style="zoom:50%;" /></p><blockquote><p><strong>定理</strong>：若图 $G(V,E)$ 有 $n$ 个顶点，那么它的团数满足：<br>$$<br>w(G) \geq \sum_{v\in V} \frac{1}{n-d(v)}<br>$$<br><strong>证明</strong>： 首先选择一个随机的顶点排列 $\pi = v_1,v_2,v_3,…,v_n$， 每种排列出现的概率均为 $\frac{1}{n!}$. </p><p>接着考虑集合 $C_{\pi}$, 如果 $v_i$ 和和排列在它之前的所有点都是 $v_j(j&lt;i)$邻接的，就将 $v_i$ 选入集合 $C_{\pi}$ 中. </p><p>根据以上定义， $C_{\pi}$ 即为 $G$ 的一个 Clique. </p><p>令 $X= |C_{\pi}|$ 为对于的随机变量，我们有：<br>$$<br>X = \sum_{i=1}^n X_i<br>$$<br>其中 $X_i$ 是 顶点 $v_i$ 的指示变量，即 $X_i=1 \space or \space 0$ 决定了 $v_i \in C_{\pi} \space or \space v_i \notin C_{\pi}$</p><p>需要注意，当 $v_i \in C_{\pi}$ 时，意味着在当前排列中，$v_{i}$ 要排在其它 $n-1-d(v_i)$ 个和它不相连的点的前面，则对于点 $v_i$ 和这些不相连的点共 $n-d(v_i)$ 个点，每个点排在最前面的概率是相等的，所以 $v_i$ 排在最前的概率为：<br>$$<br>\frac{1}{n-d(v_i)}<br>$$<br>该概率也即顶点 $v_i$ 属于 $C_{\pi}$ 的概率；</p><p>所以 $C_{\pi}$ 中顶点数量的数学期望为：<br>$$<br>\sum_{v\in V} \frac{1}{n-d(v)}<br>$$</p></blockquote><p>​    </p><ul><li><strong>思考题</strong>： 设 $F$ 为 $N=\{1,2,…,n\}$ 子集合的簇, 且假设不存在 $A,B\in F$ 满足 $A\subset B$. 令 $\sigma \in S_n$ 为元素数量为 $N$ 的随机排列，并考虑以下随机变量：</li></ul><p>$$<br>X = |\{i:\{\sigma(1),\sigma(2),…,\sigma(i)\}\in F|<br>$$</p><p>请通过计算 $X$ 的期望来证明 $|F|\leq \left(\begin{array}{c}<br>n \\<br>\lfloor n / 2\rfloor<br>\end{array}\right)$</p><blockquote><p><strong>证明一（概率方法）</strong>：$\sigma_n$ 即对 $1,2,…,n$ 的重新排列(置换)：$\sigma(1),\sigma(2),…,\sigma(n)$ , 共有 $n!$ 种排列&gt;的可能</p><p>设随机排列 $\{\sigma(1),\sigma(2),…,\sigma(i)\} \subseteq F$, 假设 $F$ 中 $i$ 元子集合个数为 $k_i$, &gt;则 $\sum_{i=1}^{n} k_i = |F|$</p><p>由以上假设，可以写出对于任一随机排列 $\sigma_n={\sigma(1),…,\sigma(n)}$, 其中 $\{\sigma(1),\sigma(2),…,\sigma(i)\} \subseteq F$ 的概率为：</p><p>$$<br>k_i \frac{i!(n-i)!}{n!}<br>$$</p><p>对于 $F$ 中任一 $i$ 元子集合，能满足条件的 $\sigma_i$ 有 $i!$ 种，再选取后 $n-i$ 个序列置换构成 $\sigma_n$，则共有 $i!(n-i)!$ 种“有利事件”；而 $\sigma_n$ 的可能共有 $n!$ 种(事件总数)，则得到以上结果。</p><p>所以，对于所有的 $i$， 由于不存在 $A,B \in F$ 使得 $A\subset B$, 这意味着统计的“有利事件”之间没有交叉，则以下不等式成立：<br>$$<br>\sum_{i=1}^n k_i\frac{i！(n-i)!}{n!} \leq 1<br>$$</p><p>化简得<br>$$<br>\frac{\sum_{i=1}^{n}k_i}{\left(\begin{array}{c}<br>n \\<br>\lfloor n / 2\rfloor<br>\end{array}\right)} \leq \sum_{i=1}^n \frac{k_i}{C_n^i} \leq 1<br>$$<br>即：<br>$$<br>\sum_{i=1}^n k_i \leq \left(\begin{array}{c}<br>n \\<br>\lfloor n / 2\rfloor<br>\end{array}\right)<br>$$</p><p><strong>证明二（计数思想）</strong>: </p><p>假设一种集合包含关系的链： $\{0\}\subseteq A_1 \subseteq A_2 \subseteq … \subseteq X$易知这种最长链一共存在 $n!$ 条;</p><p>又设集合 $A,B\in F$, 那包含集合 $A$ 的最长链  $\{0\}\subseteq A_1 \subseteq… \subseteq A \subseteq … \subseteq X$ 的数目为:<br>$$<br>|A|·(|A|-1)·(|A|-2)···1 · (n-|A|)! = |A| ! (n-|A|)!<br>$$<br>同理包含集合 $B$ 的最长链的数目为：<br>$$<br>|B|!(n-|B|)!<br>$$<br>由于不存在 $A,B \in F$ 使得 $A\subset B$， 所以这两个的最长链不存在交叉，则有：<br>$$<br>\sum_{A\in F} |A| (n-|A|)! \leq n!<br>$$<br>化简得：<br>$$<br>\sum_{A\in F} |A| \leq \left(\begin{array}{c}n \\\ \lfloor n / 2\rfloor\end{array}\right)<br>$$</p></blockquote><hr><h4 id="8-色素数量和围长"><a href="#8-色素数量和围长" class="headerlink" title="8.  色素数量和围长"></a>8.  色素数量和围长</h4><p>Note: 该定理的完整说明与证明可见于书籍：<a href="https://link.springer.com/book/10.1007/978-3-662-57265-8" target="_blank" rel="noopener"><em>Proofs from THE BOOKS</em></a> -P314</p><p>考虑图 $G$, 它有 $n$ 个顶点且色素数量为 $\chi(G)$. 如果 $\chi(G)$ 很大，也就是说若我们需要很多种颜色去染色，那么我们是否可以推测出 $G$ 可能包含一个非常大的完全子图呢？事实上，这是不成立的。我们是否能够定义不存在一个较小长度的cycle, 但仍然拥有任意大的色素数量的图呢？这是完全可以做到的，为了方便证明，我们称图 $G$ 中最短cycle的长度为”<strong>围长</strong> (girth)”, 记作 $\gamma(G)$.  则有以下定理：</p><blockquote><p><strong>定理</strong>：对于 $k\geq 2$, 存在一个图 $G$ , 它的色素数量 $\chi(G) &gt;k$ 且 围长 $\gamma(G)&gt;k$</p></blockquote><blockquote><p>证明思路：设事件 $A: \chi(G)\leq k$ , 事件 $B: \gamma(G)\leq k$ , 要是能够证明 $P(A)&lt;\frac{1}{2}, P(B)&lt;\frac{1}{2}$, 则有 $P(A \cup B) \leq P(A)+P(B)&lt;\frac{1}{2} + \frac{1}{2} = 1$, 故 $P(\overline{A} \cap \overline{B}) = 1-P(A \cup B) &gt; 0$ ，这样就能够证明 $\chi(G) &gt; k$ 且 $\gamma(G)&gt;k$ 的存在性。（概率方法中一般考虑“坏”事件，要是坏事件发生的概率小于 1， 那么好事件发生的概率一定大于 0）</p><p>所以我们将问题转换为证明 $P(\chi(G)\leq k) \leq \frac{1}{2}$</p><p><strong>证明：</strong> 令 $V = \{v_1,v_2,…,v_n\}$ 为顶点集合，$p$ 为0~1之间的固定概率。 根据概率 $p$ 对两顶点间进行连线，构建<strong>随机图</strong> $G(n,p)$ ，它包含 $V$ 上所有可能的图。 </p><p>根据以上定义，得到一个完全图 $K_n$ 的概率为 $p^{C_n^2}$ （$n$ 个顶点间共可有 $C_n^2$ 条可连边）</p><ul><li><p>首先我们对色素数量 $\chi(G)$ 进行研究。我们已知 $\chi(G)·\alpha(G) \geq n$, 因此，如果 $\alpha(G)$ 相比于 $n$ 而言很小，那么 $\chi(G)$ 就必然很大。所以我们可以从色素数量转而研究独立数。 </p><p>假设 $2\leq r\leq n$，那么 $V$ 中任一固定的 $r$ 顶点集合为独立集的概率为 $(1-p)^{C_r^2}$ , 我们能得出以下结论：<br>$$<br>P(\alpha \geq r) \leq C_n^r (1-p)^{C_r^2} \leq n^r(1-p)^{C_r^2} = (n(1-p)^{\frac{r-1}{2}})^r \leq (ne^{\frac{-p(r-1)}{2}})^r<br>$$</p><blockquote><ol><li>如何理解 $P(\alpha \geq r) \leq C_n^r (1-p)^{C_r^2}$ ?</li></ol><ul><li><p>$P(\alpha \geq r) = P(A_1 \cup A_2 \cup …\cup A_{C_n^r}) ~ \leq ~\sum P(A_i) = C_n^r (1-p)^{C_r^2}$ </p></li><li><p>其中 $A_i$ 表示第 $i$ 个 $r$ 元子集合为独立集</p></li></ul><ol start="2"><li>$1-p \leq e^{-p}$</li></ol></blockquote><p>接下来我们想要证明的就是 $P(\alpha \geq r)$ 接近于 0</p><p>给定任意固定的 $k&gt;0$,  我们现在选择 $p := n ^{-\frac{k}{k+1}}$ , 接下来证明当 $n$ 足够大时，有：<br>$$<br>P(\alpha &gt; \frac{n}{2k})&lt;\frac{1}{2} \tag{1}<br>$$</p><blockquote><p>为什么这里 $r$ 选为 $\frac{n}{2k}$?</p><ul><li>因为有 $\chi(G)·\alpha(G)\geq n$, 若能证明 $\alpha(G) &lt;\frac{k}{n}$, 则 $\chi(G) &gt; k$; 而后面还要考虑 $\gamma(G)$, 需要删除点，所以这里除以了2</li></ul></blockquote><p>因为函数 $n^\frac{1}{k+1}$  的增长速度比 $log(n)$ 快，则当 $n$ 较大时，有 $n^\frac{1}{k+1}\geq 6klog(n)$, 因此 $p\geq 6k\frac{log(n)}{n}$ . 对于 $r := \lceil \frac{n}{2k} \rceil$ 有 $pr \geq 3log(n)$, 因此有：<br>$$<br>n e^{-p(r-1) / 2}=n e^{-\frac{p r}{2}} e^{\frac{p}{2}} \leq n e^{-\frac{3}{2} \log n} e^{\frac{1}{2}}=n^{-\frac{1}{2}} e^{\frac{1}{2}}=\left(\frac{e}{n}\right)^{\frac{1}{2}}<br>$$<br>当 $n \rightarrow \inf$, 上式收敛至 $0$ . </p><p>所以存在 $n_1$, 当 $n\geq n_1$ 时，  下式成立“<br>$$<br>P(\alpha &gt; \frac{n}{2k})&lt; \frac{1}{2}<br>$$</p></li><li><p>现在我们对第二个参数 $\gamma(G)$ 进行分析. 对于给定的 $k$ 值，我们希望证明不存在很多的长度 $\leq k$  的cycle.  设两点间连线的概率为 $p$,  $3 \leq i \leq k$, 则长度为 $i$ 的 cycle 出现的概率为 $p^i$. 将所有长度 $\leq k$ 的 cycle的数量记作 $X$， 而对于 $i$ 个点，它们组成一个 cycle的方式共有 $\frac{(i-1)!}{2}$ 种， 则 $X$ 的数学期望为：<br>$$<br>E(X) = \sum_{i=3}^k \left(\begin{array}{c}<br>n \\<br>i<br>\end{array}\right) \frac{(i-1)!}{2} p^i \leq \frac{1}{2}\sum_{i=3}^{k}n^ip^i\leq\frac{1}{2}(k-2)n^kp^k<br>$$<br>应用Markov不等式（$P(X\geq a)\leq \frac{EX}{a}$）, 可得：<br>$$<br>P(X\geq \frac{n}{2}) \leq \frac{E(X)}{n/2} \leq (k-2)\frac{(np)^k}{n} = (k-2)n^{-\frac{1}{k+1}}<br>$$</p><p>当 $n$ 趋于无穷时，等式右端趋于 0，所以存在 $n_2$ 使得当 $n\geq n_2$ 时，有：</p></li></ul><p>$$<br>    P (X\geq \frac{n}{2}) &lt; \frac{1}{2}<br>$$</p><p>综上，当 $n\geq max(n_1,n_2)$ 时，总存在一个基于这 $n$ 个顶点的图 $H$ 有：$\alpha(H) &lt; \frac{n}{2k}$,  且长度小于 $k$ 的cycle 的数量少于 $\frac{n}{2}$ 个。通过    <strong>删除</strong>这些cycle中的某一个顶点，即<strong>最多删除</strong> $\frac{n}{2}$ 个顶点，我们就能够得到一个<strong>最多含有</strong> $\frac{n}{2}$ 顶点的图 $G$，它总满足<br>$$<br>\gamma(G)&gt;k<br>$$</p><p>​    又因为 $ \alpha(G) \leq \alpha(H) &lt; \frac{n}{2k}$ ，则有：</p><p>$$<br>\chi(G) \geq \frac{n/2}{\alpha(G)} \geq \frac{n}{2\alpha(H)} &gt; \frac{n}{n/k} =k<br>$$</p><p>证毕</p></blockquote><h4 id="9-去随机化-De-randomization"><a href="#9-去随机化-De-randomization" class="headerlink" title="9. 去随机化  De-randomization"></a>9. 去随机化  De-randomization</h4><p>去随机化：将随机概率转化为一个<strong>确定性的算法</strong> （存在性 $\rightarrow$ 确定算法 ）</p><h5 id="9-1-染色问题"><a href="#9-1-染色问题" class="headerlink" title="9.1 染色问题"></a>9.1 染色问题</h5><blockquote><p><strong>定理</strong>：使用两种颜色对完全图 $K_n$ 进行边染色，<strong>最多</strong>只会有 $\left(\begin{array}{c}n \\4\end{array}\right) 2 ^{-5}$ 个同色（monochromatic）的 $K_4$ (4 顶点完全图)</p></blockquote><blockquote><p><strong>证明一 （数学期望） </strong>：</p><p>$2^{-5}$ : 六条边染同一种颜色的概率为 $2 ·(\frac{1}{2})^{6} = 2^{-5}$</p><p>$C_n^4:n$ 个顶点中选取 4 个</p><p>综上，同色 $K_4$ 的数量的<strong>数学期望</strong>为 $\left(\begin{array}{c}n \\4\end{array}\right) 2 ^{-5}$</p></blockquote><p>那么如何进行染色，使得含有不超过 $\left(\begin{array}{c}n \\4\end{array}\right) 2 ^{-5}$ 个同色 $K_4$ 呢？$\rightarrow$ 去随机化 $\rightarrow$ <strong>条件概率法</strong></p><blockquote><p><strong>证明二 （去随机化） </strong>：</p><p>我们首先为所有的已经部分染色了的 $K_n$ 赋予不同的权值 $w$ : 给定当前的染色方案，令 $K$ 表示 $K_n$ 中的 $K_4$ </p><ul><li><p>如果已经存在不同色的边，那么令 $w(K)=0$; </p></li><li><p>如果尚没有边被染色，则 $w(K)=2^{-5}$, </p></li><li><p>如果 $K$ 中已经有 $r\geq1$ 条边染了相同的颜色，则令 $w(K)=2^{r-6}$</p></li></ul><p>此外，我们定义 $K_n$ 的所有局部染色方案的权重 $W = \sum w(K)$, 其中 $K$ 包含了 $K_n$ 中所有 $K_4$ 的情况 .</p><p>对 $K_n$ 的 $\left(\begin{array}{c}n \\2\end{array}\right)$ 条边任意排序：$e_1,e_2,…,e_{C_n^2}$,  然后按照边的排序构造我们期望的染色方案。假设 $e_1,e_2, …,e_{i-1}$ 已经根据染色方案 $c$ 进行染色，此时 $K_n$ 的权重为 $W$, 接下来需要对边 $e_i$ 进行染色；令 $W_{黑}$ 表示 $e_i$ 染黑色的话 $K_n$ 的权重， $W_{白}$ 表示染白色的权重，则有：<br>$$<br>W = \frac{W_黑+W_白}{2}<br>$$<br>据此，要确定 $e_i$ 到底染何种颜色，我们只需要比较 $W_黑$ 和 $W_白$ 的大小，从而<strong>选取更小的颜色进行染色</strong></p><ul><li>若 $W_黑$ 比较小，则有：$W = \frac{W_黑+W_白}{2} \geq W_黑 $ ，这意味着 $W$ 在染色的过程中不断变小/不会增长，我们想要求的是一个尽可能少的同色 $K_4$, 则选取更小的值进行染色</li><li>如果我们想要得到的越多越好的同色 $K_4$, 则应该选取权值更大的颜色进行染色</li></ul><p>在染色开始之前，同色的 $K_4$ 的数量的期望为 $\left(\begin{array}{c}n \\4\end{array}\right) 2 ^{-5}$, 而根据这种策略进行染色，后续的数量只会不断小于这个数字。染色结束后， $K_n$ 的权重即为同色 $K_4$ 的数量。 </p></blockquote><h5 id="9-2-Tenure-Game"><a href="#9-2-Tenure-Game" class="headerlink" title="9.2 Tenure Game"></a>9.2 Tenure Game</h5><p>Tenure Game 是一种有趣的博弈游戏：在一家公司有 $Paul $和 $Carole $两名员工，每年 $Paul$ 会提供给 $Carole$ 一份待升职的员工名单 $L$，而 $Carole$ 拿到名单后有两种选择：</p><p>(1) 对名单上所有员工提升一级，但不在名单上的员工全解雇</p><p>(2) 名单上员工全解雇，不在名单上的员工升一级 </p><p>（该公司职位由高到低分为很多等级）</p><p>$Paul$ 赢得游戏的条件是有人得到了最高的职位，而没人得到最高职位时则 $Carole$ 赢.</p><p><strong>问题</strong>：假设当前距离最高职位只差 $i$ 级提升的员工人数为 $a_i$, $1\leq i\leq k$, 问无论 $Paul$ 如何提供名单 $L$， $Carole$ 怎么样才能一直赢下去？</p><p>例子，若 $a_1=2$, 则 $Paul$ 只需要在名单上包含其中任意一人， $Carole$ 都会输</p><blockquote><p><strong>结论</strong>：当 $\sum a_k2^{-k}&lt;1$ 时，$Carole$ 一定会赢 </p></blockquote><blockquote><p><strong>证明一（数学期望）：</strong>假设 $Carole$ 做出两种选择的概率均为$1/2$ ，则易知 $\sum a_k2^{-k}$ 为得到最高职位的人数 $T$ 的数学期望（令每个职员升降职的指示变量为 $I_f$, 则 $T=\sum I_f, ~E(T)=\sum E[I_f]=\sum a_k2^{-k}$,  ），当 $E(T)&lt;1$ 时，$Carole$ 都会赢</p><p><strong>证明二（去随机化）</strong>：设 $Carole$ 随机地进行游戏，最终最高职位的人数的期望为 $E(T)$， 将该期望视作权重。现在 $Paul$ 提供给 $Carole$ 了一份名单，令 $T_1$ 为 $Carole$ 当前选择 (1) 后续随机游戏的情况下，最终获得最高职位的人数，$T_2$ 为选择 $(2)$ 的人数。</p><p>因为： $E(T) = \frac{E(T_1)+E(T_2)}{2}$</p><p>则 $Carole$ 能够赢下游戏的关键就是选择期望更小的选项：选择 (1) 如果有 $E(T_1)\leq E(T_2)$, 否则选择 (2)</p><p>这样选择能够保证 $E(T)$ 不断减小，游戏起始时有 $E(T)\leq 1$, 则最终游戏结束时也必有 $E(T)&lt; 1$</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 图论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图论 </tag>
            
            <tag> 概率方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记-Learn to pay attention</title>
      <link href="/learn-to-pay-attention/"/>
      <url>/learn-to-pay-attention/</url>
      
        <content type="html"><![CDATA[<p>注：部分转载于<a href="https://www.jianshu.com/p/d5c968e0e194" target="_blank" rel="noopener">https://www.jianshu.com/p/d5c968e0e194</a>（博主：月牙眼的楼下小黑）</p><p>发表于：<strong><code>ICLR 2018 (Poster)</code></strong><br>代码地址：<a href="https://github.com/SaoYan/LearnToPayAttention" target="_blank" rel="noopener">https://github.com/SaoYan/LearnToPayAttention</a></p><p>论文类型：<strong>注意力机制</strong>用于分类网络</p><hr><p><strong>摘要</strong>：提出一种注意力生成机制，使用全局特征（global feature）为不同尺度上的特征图（local feature）生成Attention Map， 强制网络仅使用不同尺度的Local feature结合对应的Attention Map进行最终分类，并使得不同scale的Attention map关注图像不同的区域，互相补充。</p><hr><p><strong>亮点</strong>： </p><ul><li>全局特征被视作一种<strong>Query</strong>, 直接影响Attention maps的生成</li><li>不同scale的Attention map关注图像不同的区域，互相补充</li><li>强制网络仅使用local features进行分类</li><li>使用了不同尺度上的特征进行融合</li></ul><hr><p><strong>模型概览</strong>：<br><img src="https://upload-images.jianshu.io/upload_images/18474746-8bffa6bd8f971d0b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="注意力机制"></p><p><img src="https://upload-images.jianshu.io/upload_images/18474746-8a0d27be9757ea8e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="VGG网络流程"></p><p><strong>流程概述</strong>：</p><h2 id="如上图所示，L-L1、L2、L3-分别为VGG网络中不同尺度下的中间特征输出（local-feature，原属于VGG网络的最终分类层FC-2-10被移除，全连接层FC-1-512的输出G即被视作全局特征-（global-feature），Attention-Estimator接收Ln和G作为输入，计算出Attention-map-Attention-map作用于Ln的每一个channel得到-Weighted-local-feature-L-39-n。各个scale下的L-39-n进行concat之后得到L-39-L-39-1、L-39-2、L-39-3-，最后将L-39-送入全连接层分类器FC-2-10进行最终分类。"><a href="#如上图所示，L-L1、L2、L3-分别为VGG网络中不同尺度下的中间特征输出（local-feature，原属于VGG网络的最终分类层FC-2-10被移除，全连接层FC-1-512的输出G即被视作全局特征-（global-feature），Attention-Estimator接收Ln和G作为输入，计算出Attention-map-Attention-map作用于Ln的每一个channel得到-Weighted-local-feature-L-39-n。各个scale下的L-39-n进行concat之后得到L-39-L-39-1、L-39-2、L-39-3-，最后将L-39-送入全连接层分类器FC-2-10进行最终分类。" class="headerlink" title="如上图所示，L: [L1、L2、L3]分别为VGG网络中不同尺度下的中间特征输出（local feature，原属于VGG网络的最终分类层FC-2, 10被移除，全连接层FC-1, 512的输出G即被视作全局特征 （global feature），Attention Estimator接收Ln和G作为输入，计算出Attention map, Attention map作用于Ln的每一个channel得到 Weighted local feature L&#39;n。各个scale下的L&#39;n进行concat之后得到L&#39;:[L&#39;1、L&#39;2、L&#39;3]，最后将L&#39;送入全连接层分类器FC-2, 10进行最终分类。"></a>如上图所示，<code>L: [L1、L2、L3]</code>分别为VGG网络中不同尺度下的中间特征输出（local feature，原属于VGG网络的最终分类层<code>FC-2, 10</code>被移除，全连接层<code>FC-1, 512</code>的输出<code>G</code>即被视作全局特征 （global feature），<code>Attention Estimator</code>接收<code>Ln</code>和<code>G</code>作为输入，计算出Attention map, Attention map作用于<code>Ln</code>的每一个channel得到 Weighted local feature <code>L&#39;n</code>。各个scale下的<code>L&#39;n</code>进行concat之后得到<code>L&#39;:[L&#39;1、L&#39;2、L&#39;3]</code>，最后将<code>L&#39;</code>送入全连接层分类器<code>FC-2, 10</code>进行最终分类。</h2><p><strong>关键操作和细节</strong>：</p><ul><li>Attention map 具体如何计算得到？<br><code>论文中提出了两种方法通过L和G来计算Attention map</code>：<br><strong><code>1.有参法(parameterised)</code></strong>：<code>将两个张量逐元素相加后，再经过一个线性映射（1x1的卷积）学习Attention map， 下式中u即代表学习到的线性映射</code><br><img src="https://upload-images.jianshu.io/upload_images/18474746-d372dd85afc0a1d2.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="有参法"><br><strong><code>2.点乘法(dot-product-based)</code></strong>: <code>将两个张量直接做点乘得到Attention map</code><br><img src="https://upload-images.jianshu.io/upload_images/18474746-557223ae032f6b7e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="点乘法"></li><li>Attention Estimator 的输入为维度不一定相同的<code>G</code>和<code>L</code>/不同的中间输出特征<code>G</code>之间维度不同，如何处理？<br><code>使用映射函数(project function)，将特征映射到相同维度(相同通道数)，具体的操作即为使用1x1的卷积对特征进行升/降维，论文源码将中间层输出特征都映射为和全局特征维度一致。</code></li></ul><hr><p><strong>思想</strong>：<br>1.利用Attention map来确认和寻找那些有效支持CNN模型做出选择的区域（这和CAM的想法类似），这个方法的一个重要前提假设是：<code>施加更多的关注在图像的显著性区域上，同时减轻对那些不相关或者易混淆区域的关注</code>是对分类有益的；<code>对图像更集中和简洁的使用更有助于数据分布发生变化时网络的鲁棒性</code>。<br>2.本文中Attention map其实是local feature和global feature间的<code>compatibility</code>的表现形式，两类特征的<code>compatibility（兼容性）</code>由上文提到的有参法或者点乘法得到。每个<code>compatibility</code>作为中间特征的注意力权重。<br>3.通过强制使用中间特征输出的集合进行分类，强制使得网络去学习解决当前任务的特定的Attention模式。</p><hr><p><strong>实验</strong>：<br>作者分别在分类任务、细粒度图像分类任务、弱监督分割任务、对抗样本攻击、跨领域图像分类任务上进行了实验，均取得了不错的结果。</p><ul><li>细粒度图像分类：作者在鸟类数据集上进行实验，不同scale的Attention区域关注鸟的不同身体部位。</li><li>弱监督分割任务：不同特征图的Attention map关注目标的不同区域，互相补充，多张Attention map能够更精准、更完整地覆盖目标区域。</li><li>对抗样本攻击：模型对对抗样本的鲁棒性更强了。</li><li>跨领域图像分类：模型特征提取能力更易迁移。</li></ul><p>除此之外，作者还对生成Attention map的两种方法进行了实验分析，分析了两种方法的不同。<br><img src="https://upload-images.jianshu.io/upload_images/18474746-0946f9a603d589e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Query-driven attention"><br>如上图所示，先利用一幅 <code>target image</code>的 global vector 和 local feature map 产生 attention map (第三、六列), 然后再选取另一张图片作为 <code>query image</code>, 利用 <code>query image</code> 的 global vector 和 target image 的 local feature map 产生 attention map( 第四、七列) ， 比较两个attention map 的异同（第五、八列）。</p><ul><li>对于<code>点乘法</code>产生Attention map来说，global feature直接影响Attention map的生成，会使得网络关注和<code>query image</code>中目标相同类别的目标区域；</li><li>而对于<code>有参法</code>来说，global feature对Attention map几乎没有影响，作者得出的结论是有参法学习到的映射函数的权重<code>u</code>能够去学习目标为中心的高阶特征，这些特征能够较好地进行从训练数据泛化到验证数据，因为它们的类别相似；还有一个原因就是<code>global feature</code>和<code>local feature</code>在数值量级上差距过大。</li></ul><hr><p><strong>一些启示</strong>：<br>一开始看到这篇论文并没有觉得非常solid或者有什么insight，在仔细阅读完整篇论文之后有所改观。作者不仅仅是使用不同尺度的中间特征来进行分类，而是以<code>global feature</code>作为一种<code>Query</code>来控制中间特征需要关注的区域。<br>还有一个值得注意的点：作者提到全局特征和局部特征数值量级差距过大，这需要我在实验中被进一步证实，如果属实，那在对中间特征和全局特征进行融合或者拼接时应该考虑进行归一化操作。</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
